### Requirements and Installation, install sentencepiece package
```bash
pip3 install sentencepiece
```


### install fairseq package
```bash
git clone https://github.com/imonlius/fairseq.git
cd fairseq
pip3 install --editable .
```


### process_MHC_pep.py:
tokenize the pretraining data and fine-tuning data
```bash
python3 process_MHC_pep.py
```


### shuffle_and_split_pretrain.sh:
Shuffle and split pretraining data file into training, validation, and test data files.

```bash
bash shuffle_and_split_pretrain.sh preprocessed_data/uniprot_pretraining_data.csv \
	pretraining_data/split_tokenized/ \
	pretraining
```

- Arguments:

| Name | Description | Example |
| ----- | ------------------------------------------ | ------ |
| INPUT | Input file | tokenized input directory |
| OUTPUT | Output directory | output directory |
| PREFIX | Prefix for output files | pretraining |


### Preprocess/binarize pretraining data:
```bash
fairseq-preprocess \
	--only-source \
	--trainpref pretraining_data/split_tokenized/pretraining.split.train.80 \
	--validpref pretraining_data/split_tokenized/pretraining.split.valid.10 \
	--testpref pretraining_data/split_tokenized/pretraining.split.test.10 \
	--destdir pretraining_data/split_binarized \
	--workers 60
```


### pRoBERTa_pretrain.sh
 Pre-train RoBERTa model

```bash
bash pretrain.sh pretrain 2 pretrained_model \
        pretraining/split_binarized/ \
        768 5 125000 3125 0.0025 32 64 3
```

- Arguments

| Name | Description | Example |
| ----- | ------------------------------------------ | ------ |
| PREFIX | Prefix for the model output files | pretrain |
| NUM_GPUS | Number of GPUs to be used during pretraining | 2 |
| OUTPUT_DIR | Output directory | Model output directory |
| DATA_DIR | Binarized input data directory | Binarized input data directory |
| ENCODER_EMBED_DIM | Dimension of embedding generated by the encoders | 768 |
| ENCODER_LAYERS | Number of encoder layers in the model | 5 |
| TOTAL_UPDATES | Total (maximum) number of updates during training | 125000 |
| WARMUP_UPDATES | Total number of LR warm-up updates during training | 3125 |
| PEAK_LEARNING_RATE | Peak learning rate for training | 0.0025 |
| MAX_SENTENCES | Maximum number of sequences in each batch | 32 |
| UPDATE_FREQ | Updates the model every UPDATE_FREQ batches | 64 |


### shuffle_and_split.sh:
Shuffle and split fine-tuning data file into training, validation, and test data files.

```bash
bash shuffle_and_split.sh  preprocessed_data/tokenized_fine_tuning_data.csv \
	preprocessed_data/split_tokenized/full/ \
	tokenized_fine_tuning_data
```

- Arguments:

| Name | Description | Example |
| ----- | ------------------------------------------ | ------ |
| INPUT | Input file | tokenized input directory |
| OUTPUT | Output directory | output directory |
| PREFIX | Prefix for output files | tokenized_data |


### Preprocess/binarize MHC_pep data:
```bash
bash preprocess_MHC-pep.sh
```


### pep_MHC_interaction.sh:
Fine-tuned model for pep_MHC Interaction Prediction Task

#### Example Usage:
```bash
bash pep_MHC_interaction.sh pep_MHC_interaction 2 pep_MHC_interaction_prediction \
        preprocessed_data/split_binarized \
        768 5 12500 3125 0.0025 32 64 1 3 \
        pretrained_model/pretrain.DIM_768.LAYERS_5.UPDATES_125000.WARMUP_3125.LR_0.0025.BATCH_4096.PATIENCE_3/checkpoints/checkpoint_best.pt \
      no 1
```

- Arguments

| Name | Description | Example |
| ----- | ------------------------------------------ | ------ |
| PREFIX | Prefix for the model output files | pep_MHC_interaction |
| NUM_GPUS | Number of GPUs to use for finetuning | 2 |
| OUTPUT_DIR | Model output directory |
| DATA_DIR | Binarized input data directory |
| ENCODER_EMBED_DIM | Dimension of embedding generated by the encoders | 768 |
| ENCODER_LAYERS | Number of encoder layers in the model | 5 |
| TOTAL_UPDATES | Total (maximum) number of updates during training | 12500 |
| WARMUP_UPDATES | Total number of LR warm-up updates during training | 3125 |
| PEAK_LEARNING_RATE | Peak learning rate for training | 0.0025 |
| MAX_SENTENCES | Maximum number of sequences in each batch | 32 |
| UPDATE_FREQ | Updates the model every UPDATE_FREQ batches | 64 |
| NUM_CLASSES | number of classes | 1 |
| PATIENCE | Early stop training if valid performance doesnâ€™t improve for PATIENCE consecutive validation runs | 3 |
| PRETRAIN_CHECKPOINT | Path to pretrained model checkpoint |
| RESUME_TRAINING | Whether to resume training from previous finetuned model checkpoints | no |
| USE-CLS-TOKEN | Use [cls] token instead of feature vector sum for RobertaClassificationHead | 1 |


### evaluate_MHC_pep_model.py: 
Predict pep_MHC_interaction using fine-tuned RoBERTa model

```bash
python3 evaluate_MHC_pep_model.py preprocessed_data/split_tokenized/full/tokenized_data.split.test.10 \
	preprocessed_data/split_binarized/ \
	predictions.tsv \
	pep_MHC_interaction_prediction/pep_MHC_interaction.DIM_768.LAYERS_5.UPDATES_12500.WARMUP_3125.LR_0.0025.BATCH_4096.PATIENCE_3/checkpoints/ \
	pep_MHC_interaction_prediction 256
```

- Arguments:

| Name | Description | Example |
| ----- | ------------------------------------------ | ------ |
| DATA | Path to input examples to predict. This should be formatted as a CSV with the columns, in order: tokenized to pep, tokenized to MHC, true label |
| BINARIZED_DATA | Path to binarized pep_MHC_interaction data | 
| OUTPUT | Path to output file with model predictions |
| MODEL_FOLDER | Model checkpoints folder. Will use checkpoint_best.pt file in the folder |
| CLASSIFICATION_HEAD_NAME | Name of the trained classification head | pep_MHC_interaction_prediction |
| BATCH_SIZE | Batch size for prediction | 256 |
